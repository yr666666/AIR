This code is the official implementation for the paper "Accurate and Lightweight Learning for Specific Domain Image-Text Retrieval" from the 2024 ACM Multimedia Conference. 
The paper introduces two innovations: the MLCE loss and the SPDS strategy.

Here, we use the remote sensing image-text retrieval task as an example to demonstrate how to use the code.

(1) Data Preparation: Users should modify their data paths in lines 30-38 of adds/params.py (using RSITMD as an example in the file).
(2) Model Weights Preparation: Users should modify the path to the pre-trained weights of the CLIP model in lines 45-49. We provide the CLIP weights "ViT-B-16.pt", available at: [LINK]. Users can also download it from the official CLIP implementation or Hugging Face.
(3) MLCE Loss Function Instructions: Users can view the source code in lines 22-32 of the loss.py file. Set the combination coefficient for the MLCE loss on line 19.

Recommended Hyperparameter Settings: For the MLCE loss function, the combination coefficient is 30 for the RSITMD dataset and 10 for the RSICD dataset.

Run: python -u train.py


Additional Note: The version of the code we have open-sourced is an updated version from the paper. In the open-source version, the compatible versions of Torch, CUDA, and cuDNN have been updated. We recommend using Torch 1.9.1 with CUDA 11.1 (cu111). 
We found that the results from the open-source version are very close to those reported in the paper, although the optimal hyperparameters (such as the combination coefficient for MLCE loss) might differ slightly. The open-source version can achieve slightly better results than those reported in the paper.
